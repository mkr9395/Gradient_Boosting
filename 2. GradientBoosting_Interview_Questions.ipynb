{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966ac670",
   "metadata": {},
   "source": [
    "## üîπ **Basic Level Questions (1‚Äì10)**\n",
    "\n",
    "1. **What is Gradient Boosting?**\n",
    "   Gradient Boosting is an ensemble technique that builds models sequentially, where each new model tries to correct the errors of the previous one by minimizing a loss function using gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "2. **How does Gradient Boosting work?**\n",
    "   It starts with an initial weak model, calculates residuals (errors), then fits a new model to these residuals. This process continues, and models are added until the errors are minimized.\n",
    "\n",
    "\n",
    "\n",
    "3. **What is a weak learner in Gradient Boosting?**\n",
    "   A weak learner is typically a shallow decision tree (e.g., depth 1 or 3) that performs slightly better than random guessing.\n",
    "\n",
    "\n",
    "\n",
    "4. **How is Gradient Boosting different from AdaBoost?**\n",
    "\n",
    "   * **AdaBoost** uses weighted errors and updates sample weights.\n",
    "   * **Gradient Boosting** minimizes a loss function using gradients (residuals).\n",
    "\n",
    "\n",
    "\n",
    "5. **What is the loss function in Gradient Boosting?**\n",
    "   It can vary:\n",
    "\n",
    "   * Regression: MSE or MAE\n",
    "   * Classification: Log Loss (binary), Multiclass log loss\n",
    "\n",
    "\n",
    "6. **Why do we use gradient descent in Gradient Boosting?**\n",
    "   To minimize the chosen loss function by computing its gradient with respect to predictions and adjusting the model accordingly.\n",
    "\n",
    "\n",
    "\n",
    "7. **What are residuals in Gradient Boosting?**\n",
    "   The difference between the actual value and the predicted value of the model; these are used to train the next model.\n",
    "\n",
    "\n",
    "\n",
    "8. **Why are decision trees commonly used as base learners in Gradient Boosting?**\n",
    "   They can capture nonlinear relationships, are fast to train, and handle different feature types well.\n",
    "\n",
    "\n",
    "\n",
    "9. **What is learning rate in Gradient Boosting?**\n",
    "   A hyperparameter that controls how much each model contributes to the overall prediction. Smaller values make learning more robust but slower.\n",
    "\n",
    "\n",
    "\n",
    "10. **What is shrinkage in Gradient Boosting?**\n",
    "    Another name for learning rate; it ‚Äúshrinks‚Äù the contribution of each tree to prevent overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0b789",
   "metadata": {},
   "source": [
    "## üîπ **Intermediate Level Questions (11‚Äì20)**\n",
    "\n",
    "11. **What are the main hyperparameters in Gradient Boosting?**\n",
    "\n",
    "    * Learning rate\n",
    "    * Number of estimators\n",
    "    * Max depth\n",
    "    * Subsample\n",
    "    * Min samples split / leaf\n",
    "    * Loss function\n",
    "\n",
    "\n",
    "12. **What is the effect of increasing the number of estimators?**\n",
    "    Increases model complexity. Can lead to better performance on training data but risks overfitting if not regularized properly.\n",
    "\n",
    "\n",
    "\n",
    "13. **How does Gradient Boosting prevent overfitting?**\n",
    "    Through:\n",
    "\n",
    "    * Early stopping\n",
    "    * Learning rate\n",
    "    * Regularization (e.g., limiting tree depth, min samples)\n",
    "    * Subsampling (stochastic GB)\n",
    "\n",
    "\n",
    "14. **What is stochastic gradient boosting?**\n",
    "    A variation where a random subset of training data is used at each iteration, which improves generalization and reduces overfitting.\n",
    "\n",
    "\n",
    "15. **What‚Äôs the difference between bagging and boosting?**\n",
    "\n",
    "    * **Bagging** (e.g., Random Forest): models trained independently in parallel\n",
    "    * **Boosting**: models trained sequentially with each correcting the previous\n",
    "\n",
    "\n",
    "16. **Can Gradient Boosting be used for classification?**\n",
    "    Yes, using log loss or softmax loss for binary or multiclass classification.\n",
    "\n",
    "\n",
    "17. **What is early stopping in Gradient Boosting?**\n",
    "    A technique to stop training when the validation error starts increasing, to avoid overfitting.\n",
    "\n",
    "\n",
    "18. **What are some drawbacks of Gradient Boosting?**\n",
    "\n",
    "    * Computationally intensive\n",
    "    * Prone to overfitting without tuning\n",
    "    * Requires careful hyperparameter tuning\n",
    "\n",
    "\n",
    "19. **How is feature importance calculated in Gradient Boosting?**\n",
    "\n",
    "    * Gain: improvement in loss from a feature\n",
    "    * Cover: number of samples affected\n",
    "    * Frequency: number of times a feature is used in splits\n",
    "\n",
    "\n",
    "20. **What‚Äôs the difference between GBDT and XGBoost?**\n",
    "    XGBoost is a highly optimized version of Gradient Boosting with:\n",
    "\n",
    "    * Regularization\n",
    "    * Parallelization\n",
    "    * Missing value handling\n",
    "    * Tree pruning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09f701",
   "metadata": {},
   "source": [
    "## üîπ **Advanced Level Questions (21‚Äì30)**\n",
    "\n",
    "21. **What is the role of the negative gradient in Gradient Boosting?**\n",
    "    It's used as the pseudo-residual (target) for the next model to learn.\n",
    "\n",
    "\n",
    "22. **How does Gradient Boosting handle multiclass classification?**\n",
    "    Builds one model per class using softmax or builds trees to minimize multiclass log loss.\n",
    "\n",
    "\n",
    "23. **Explain Gradient Boosting in terms of function approximation.**\n",
    "    It's a stage-wise additive model where each stage adds a function (tree) to better approximate the target function by minimizing a loss.\n",
    "\n",
    "\n",
    "24. **What are pseudo-residuals?**\n",
    "    The gradient of the loss function with respect to the predictions ‚Äî used as targets in each stage.\n",
    "\n",
    "\n",
    "25. **What is the bias-variance trade-off in Gradient Boosting?**\n",
    "\n",
    "    * Boosting reduces bias significantly\n",
    "    * But can increase variance if not regularized (e.g., too many trees)\n",
    "\n",
    "\n",
    "26. **What is the difference between XGBoost and LightGBM?**\n",
    "\n",
    "    * **XGBoost**: level-wise tree growth\n",
    "    * **LightGBM**: leaf-wise tree growth (faster but riskier for overfitting)\n",
    "\n",
    "\n",
    "27. **How does CatBoost differ from XGBoost and LightGBM?**\n",
    "\n",
    "    * Handles categorical features natively\n",
    "    * Reduces overfitting via ordered boosting\n",
    "    * Doesn‚Äôt require one-hot encoding\n",
    "\n",
    "\n",
    "28. **What is histogram-based Gradient Boosting?**\n",
    "    Converts continuous features into bins (histograms) to speed up training and reduce memory usage (used in LightGBM and sklearn's HistGradientBoosting).\n",
    "\n",
    "\n",
    "29. **When would you prefer Random Forest over Gradient Boosting?**\n",
    "\n",
    "    * When speed is critical\n",
    "    * When data is noisy or requires less tuning\n",
    "    * For quick baselines\n",
    "\n",
    "\n",
    "30. **How do you tune hyperparameters in Gradient Boosting?**\n",
    "    Use techniques like:\n",
    "\n",
    "    * Grid search or RandomizedSearchCV\n",
    "    * Bayesian optimization (e.g., Optuna)\n",
    "    * Tune in order: learning rate ‚Üí n\\_estimators ‚Üí max\\_depth ‚Üí min\\_child ‚Üí subsample ‚Üí regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807bf83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
